{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9647e15c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Could not find directory D:\\Faculdade\\Codigos\\Python\\case_cats_and_dogs\\cats_and_dogs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9268\\2700006891.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m     train_ds = image_dataset_from_directory(\n\u001b[0m\u001b[0;32m     65\u001b[0m                             \u001b[0mtrain_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                             \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m123\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\image_dataset.py\u001b[0m in \u001b[0;36mimage_dataset_from_directory\u001b[1;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mseed\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1e6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m     image_paths, labels, class_names = dataset_utils.index_directory(\n\u001b[0m\u001b[0;32m    208\u001b[0m         \u001b[0mdirectory\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\dataset_utils.py\u001b[0m in \u001b[0;36mindex_directory\u001b[1;34m(directory, labels, formats, class_names, shuffle, seed, follow_links)\u001b[0m\n\u001b[0;32m    532\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m         \u001b[0msubdirs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 534\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    535\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0msubdir\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\u001b[0m in \u001b[0;36mlist_directory_v2\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    766\u001b[0m   \"\"\"\n\u001b[0;32m    767\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_directory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 768\u001b[1;33m     raise errors.NotFoundError(\n\u001b[0m\u001b[0;32m    769\u001b[0m         \u001b[0mnode_def\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[0mop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Could not find directory D:\\Faculdade\\Codigos\\Python\\case_cats_and_dogs\\cats_and_dogs"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2 # OpenCV ou cv2 para tratamento de imagens;\n",
    "import numpy as np # Numpy para trabalharmos com matrizes n-dimensionais\n",
    "from keras.models import Sequential # Importando modelo sequencial\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D # Camada de convolução e max pooling\n",
    "from keras.layers.core import Activation, Flatten, Dense # Camada da função de ativação, flatten, entre outros\n",
    "from keras.layers import Rescaling # Camada de escalonamento\n",
    "from keras.optimizers import Adam # optimizador Adam\n",
    "from keras.callbacks import ModelCheckpoint # Classe utilizada para acompanhamento durante o treinamento onde definimos os atributos que serão considerados para avaliação\n",
    "from tensorflow.data import AUTOTUNE\n",
    "from tensorflow.keras.utils import image_dataset_from_directory # Função que carrega o dataset de um diretório\n",
    "\n",
    "def create_lenet(input_shape):\n",
    "    \"\"\"\n",
    "    Cria uma mini arquitetura lenet\n",
    "\n",
    "    Args:\n",
    "        input_shape: Uma lista de três valores inteiros que definem a forma de\\\n",
    "                entrada da rede. Exemplo: [100, 100, 3]\n",
    "\n",
    "    Returns:\n",
    "        Um modelo sequencial, seguindo a arquitetura lenet\n",
    "    \"\"\"\n",
    "    # Definimos que estamos criando um modelo sequencial\n",
    "    model = Sequential()\n",
    "\n",
    "    # Primeira camada do modelo:\n",
    "    model.add(Conv2D(20, (5, 5), padding=\"same\", input_shape=input_shape))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    # Segunda camada do modelo:\n",
    "    model.add(Conv2D(50, (5, 5), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    # Primeira camada fully connected\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500))\n",
    "    model.add(Activation(\"relu\"))\n",
    "\n",
    "    # Classificador softmax\n",
    "    model.add(Dense(classes))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_path = \"D:\\Codigos\\Python\\case_cats_and_dogs\\cats_and_dogs\" # Adicione aqui o caminho para chegar no diretório que contém as imagens de treino na sua maquina\n",
    "    models_path = \"D:\\Codigos\\Python\\case_cats_and_dogs\\Modelos\\Garbage\" # Defina aqui onde serão salvos os modelos na sua maquina\n",
    "    width = 100 # Tamanho da largura da janela que será utilizada pelo modelo\n",
    "    height = 100 # Tamanho da altura da janela que será utilizada pelo modelo\n",
    "    depth = 1 # Profundidade das janelas utilizadas pelo modelo, caso seja RGB use 3, caso escala de cinza 1\n",
    "    classes = 2 # Quantidade de classes que o modelo utilizará\n",
    "    epochs = 10 # Quantidade de épocas (a quantidade de iterações que o modelo realizará durante o treinamento)\n",
    "    init_lr = 1e-3 # Taxa de aprendizado a ser utilizado pelo optimizador\n",
    "    batch_size = 32 # Tamanho dos lotes utilizados por cada epoca\n",
    "    input_shape = (height, width, depth) # entrada do modelo\n",
    "    save_model = os.path.join(models_path, \"lenet-{epoch:02d}-{accuracy:.3f}-{val_accuracy:.3f}.model\")\n",
    "    color_mode = {1:\"grayscale\", 3: \"rgb\"} # Usado para selecionar o colormode em função da variável depth\n",
    "\n",
    "    os.makedirs(models_path, exist_ok=True)\n",
    "\n",
    "    train_ds = image_dataset_from_directory(\n",
    "                            train_path,\n",
    "                            seed=123,\n",
    "                            label_mode='categorical',\n",
    "                            validation_split=0.3,\n",
    "                            subset=\"training\",\n",
    "                            color_mode=color_mode[depth],\n",
    "                            image_size=(height, width),\n",
    "                            batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    val_ds = image_dataset_from_directory(\n",
    "                            train_path,\n",
    "                            seed=123,\n",
    "                            label_mode='categorical',\n",
    "                            validation_split=0.3,\n",
    "                            subset=\"validation\",\n",
    "                            color_mode=color_mode[depth],\n",
    "                            image_size=(height, width),\n",
    "                            batch_size=batch_size\n",
    "    )\n",
    "\n",
    "\n",
    "    rescaling_layer = Rescaling(1./255)\n",
    "    # pré-busca em buffer para que você possa produzir dados do disco sem que a E/S se torne um bloqueio\n",
    "    train_ds = train_ds.map(lambda x, y: (rescaling_layer(x), y), num_parallel_calls=AUTOTUNE)\n",
    "    val_ds = val_ds.map(lambda x, y: (rescaling_layer(x), y), num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    model = create_lenet(input_shape)\n",
    "\n",
    "    opt = Adam(lr=init_lr, decay=init_lr / epochs)\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "    model.summary()\n",
    "\n",
    "    print(\"\\n training network\")\n",
    "\n",
    "    checkpoint1 = ModelCheckpoint(save_model, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    checkpoint2 = ModelCheckpoint(save_model, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint1,checkpoint2]\n",
    "\n",
    "    H = model.fit(train_ds,\n",
    "                validation_data=val_ds,\n",
    "                epochs=epochs, \n",
    "                verbose=1,\n",
    "                callbacks=callbacks_list\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912a7625",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a418047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-gpu\n",
      "  Using cached tensorflow-gpu-2.12.0.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: python_version>\"3.7\" in c:\\users\\lucam\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (0.0.2)\n",
      "Building wheels for collected packages: tensorflow-gpu\n",
      "  Building wheel for tensorflow-gpu (setup.py): started\n",
      "  Building wheel for tensorflow-gpu (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for tensorflow-gpu\n",
      "Failed to build tensorflow-gpu\n",
      "Installing collected packages: tensorflow-gpu\n",
      "  Running setup.py install for tensorflow-gpu: started\n",
      "  Running setup.py install for tensorflow-gpu: finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py bdist_wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [18 lines of output]\n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 2, in <module>\n",
      "    File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "    File \"C:\\Users\\lucam\\AppData\\Local\\Temp\\pip-install-exzvbm78\\tensorflow-gpu_170299c462384c24b5f0960222c39078\\setup.py\", line 37, in <module>\n",
      "      raise Exception(TF_REMOVAL_WARNING)\n",
      "  Exception:\n",
      "  \n",
      "  =========================================================\n",
      "  The \"tensorflow-gpu\" package has been removed!\n",
      "  \n",
      "  Please install \"tensorflow\" instead.\n",
      "  \n",
      "  Other than the name, the two packages have been identical\n",
      "  since TensorFlow 2.1, or roughly since Sep 2019. For more\n",
      "  information, see: pypi.org/project/tensorflow-gpu\n",
      "  =========================================================\n",
      "  \n",
      "  \n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for tensorflow-gpu\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Running setup.py install for tensorflow-gpu did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [18 lines of output]\n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 2, in <module>\n",
      "    File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "    File \"C:\\Users\\lucam\\AppData\\Local\\Temp\\pip-install-exzvbm78\\tensorflow-gpu_170299c462384c24b5f0960222c39078\\setup.py\", line 37, in <module>\n",
      "      raise Exception(TF_REMOVAL_WARNING)\n",
      "  Exception:\n",
      "  \n",
      "  =========================================================\n",
      "  The \"tensorflow-gpu\" package has been removed!\n",
      "  \n",
      "  Please install \"tensorflow\" instead.\n",
      "  \n",
      "  Other than the name, the two packages have been identical\n",
      "  since TensorFlow 2.1, or roughly since Sep 2019. For more\n",
      "  information, see: pypi.org/project/tensorflow-gpu\n",
      "  =========================================================\n",
      "  \n",
      "  \n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: legacy-install-failure\n",
      "\n",
      "Encountered error while trying to install package.\n",
      "\n",
      "tensorflow-gpu\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for output from the failure.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce00d4fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'python' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18160\\4255879920.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpython\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'python' is not defined"
     ]
    }
   ],
   "source": [
    "python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d704b1a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
